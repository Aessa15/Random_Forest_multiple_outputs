{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import os\n",
    "\n",
    "# Make numpy printouts easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     C% Biomass  H% Biomass  O% Biomass  N% Biomass  C% HDPE  H% HDPE  \\\n",
       "0         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "1         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "2         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "3         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "4         63.85         8.6        3.74        0.33    85.72    14.28   \n",
       "..          ...         ...         ...         ...      ...      ...   \n",
       "242       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "243       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "244       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "245       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "246       58.00         6.3       34.00        2.40     0.00     0.00   \n",
       "\n",
       "     O% HDPE  N% HDPE  C% PS  H% PS  O% PS  N% PS  Time  Rate  Ratio  Temp  \\\n",
       "0          0      0.0    0.0    0.0    0.0      0   0.5     5   66.7   500   \n",
       "1          0      0.0    0.0    0.0    0.0      0   0.5     5  100.0   500   \n",
       "2          0      0.0    0.0    0.0    0.0      0   0.5     5   50.0   500   \n",
       "3          0      0.0    0.0    0.0    0.0      0   0.5     5   33.3   500   \n",
       "4          0      0.0    0.0    0.0    0.0      0   0.5     5    0.0   500   \n",
       "..       ...      ...    ...    ...    ...    ...   ...   ...    ...   ...   \n",
       "242        0      0.0   91.0    8.8    0.3      0  30.0   100   60.0   550   \n",
       "243        0      0.0   91.0    8.8    0.3      0  30.0   100   61.5   550   \n",
       "244        0      0.0   91.0    8.8    0.3      0  30.0   100   63.0   550   \n",
       "245        0      0.0   91.0    8.8    0.3      0  30.0   100   64.5   550   \n",
       "246        0      0.0   91.0    8.8    0.3      0  30.0   100   66.0   550   \n",
       "\n",
       "          Oil%      Char%       Gas%  \n",
       "0    60.156065  10.936392  28.907543  \n",
       "1    56.000000   0.000000  44.000000  \n",
       "2    45.993999  17.313035  36.692966  \n",
       "3    44.038082  23.267154  32.694764  \n",
       "4    25.762381  34.435228  39.802391  \n",
       "..         ...        ...        ...  \n",
       "242  16.633680  15.340660  13.907280  \n",
       "243  15.999231  14.779844  13.749581  \n",
       "244  15.349117  14.221099  13.605475  \n",
       "245  14.683338  13.664426  13.474963  \n",
       "246  14.001893  13.109825  13.358045  \n",
       "\n",
       "[247 rows x 19 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C% Biomass</th>\n      <th>H% Biomass</th>\n      <th>O% Biomass</th>\n      <th>N% Biomass</th>\n      <th>C% HDPE</th>\n      <th>H% HDPE</th>\n      <th>O% HDPE</th>\n      <th>N% HDPE</th>\n      <th>C% PS</th>\n      <th>H% PS</th>\n      <th>O% PS</th>\n      <th>N% PS</th>\n      <th>Time</th>\n      <th>Rate</th>\n      <th>Ratio</th>\n      <th>Temp</th>\n      <th>Oil%</th>\n      <th>Char%</th>\n      <th>Gas%</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>66.7</td>\n      <td>500</td>\n      <td>60.156065</td>\n      <td>10.936392</td>\n      <td>28.907543</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>100.0</td>\n      <td>500</td>\n      <td>56.000000</td>\n      <td>0.000000</td>\n      <td>44.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>50.0</td>\n      <td>500</td>\n      <td>45.993999</td>\n      <td>17.313035</td>\n      <td>36.692966</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>33.3</td>\n      <td>500</td>\n      <td>44.038082</td>\n      <td>23.267154</td>\n      <td>32.694764</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>63.85</td>\n      <td>8.6</td>\n      <td>3.74</td>\n      <td>0.33</td>\n      <td>85.72</td>\n      <td>14.28</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>500</td>\n      <td>25.762381</td>\n      <td>34.435228</td>\n      <td>39.802391</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>242</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>60.0</td>\n      <td>550</td>\n      <td>16.633680</td>\n      <td>15.340660</td>\n      <td>13.907280</td>\n    </tr>\n    <tr>\n      <th>243</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>61.5</td>\n      <td>550</td>\n      <td>15.999231</td>\n      <td>14.779844</td>\n      <td>13.749581</td>\n    </tr>\n    <tr>\n      <th>244</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>63.0</td>\n      <td>550</td>\n      <td>15.349117</td>\n      <td>14.221099</td>\n      <td>13.605475</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>64.5</td>\n      <td>550</td>\n      <td>14.683338</td>\n      <td>13.664426</td>\n      <td>13.474963</td>\n    </tr>\n    <tr>\n      <th>246</th>\n      <td>58.00</td>\n      <td>6.3</td>\n      <td>34.00</td>\n      <td>2.40</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>91.0</td>\n      <td>8.8</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>100</td>\n      <td>66.0</td>\n      <td>550</td>\n      <td>14.001893</td>\n      <td>13.109825</td>\n      <td>13.358045</td>\n    </tr>\n  </tbody>\n</table>\n<p>247 rows Ã— 19 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "\n",
    "dir_p = r'C:\\Users\\Honeyz\\Desktop\\Aessa\\THE_SIS\\PyroDataProcessed.csv'\n",
    "raw_dataset = pd.read_csv(dir_p, skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy()\n",
    "dataset = dataset.dropna()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KFold(n_splits=247, random_state=None, shuffle=False) \n",
      "\n",
      "loss for fold 1: \n",
      " Regressor 1: 5.455279628000426\n",
      " Regressor 2: 0.4264842600000076\n",
      " Regressor 3: 17.54466730999999\n",
      "\n",
      "\n",
      "loss for fold 2: \n",
      " Regressor 1: 2.4779080423196618\n",
      " Regressor 2: 0.9055405987599957\n",
      " Regressor 3: 43.09445940124\n",
      "\n",
      "\n",
      "loss for fold 3: \n",
      " Regressor 1: 4.435652509999848\n",
      " Regressor 2: 0.08896149999995018\n",
      " Regressor 3: 19.468892609999955\n",
      "\n",
      "\n",
      "loss for fold 4: \n",
      " Regressor 1: 0.08180791800018028\n",
      " Regressor 2: 0.29174981999995\n",
      " Regressor 3: 9.719359249999947\n",
      "\n",
      "\n",
      "loss for fold 5: \n",
      " Regressor 1: 1.9477793460002495\n",
      " Regressor 2: 1.2156290599999267\n",
      " Regressor 3: 6.5827917899999235\n",
      "\n",
      "\n",
      "loss for fold 6: \n",
      " Regressor 1: 1.2589031992202067\n",
      " Regressor 2: 0.6100051673998905\n",
      " Regressor 3: 6.436500847399891\n",
      "\n",
      "\n",
      "loss for fold 7: \n",
      " Regressor 1: 0.7578652225797704\n",
      " Regressor 2: 0.40413153860001216\n",
      " Regressor 3: 6.186481901399986\n",
      "\n",
      "\n",
      "loss for fold 8: \n",
      " Regressor 1: 0.8316742985998467\n",
      " Regressor 2: 0.43584488200014704\n",
      " Regressor 3: 6.853314477999852\n",
      "\n",
      "\n",
      "loss for fold 9: \n",
      " Regressor 1: 0.7541412277199129\n",
      " Regressor 2: 0.40245179240025664\n",
      " Regressor 3: 7.525487727599742\n",
      "\n",
      "\n",
      "loss for fold 10: \n",
      " Regressor 1: 0.7458192000002271\n",
      " Regressor 2: 0.4095014399999499\n",
      " Regressor 3: 8.10325856000005\n",
      "\n",
      "\n",
      "loss for fold 11: \n",
      " Regressor 1: 0.8435957760001855\n",
      " Regressor 2: 0.47941631999990264\n",
      " Regressor 3: 8.570010560000096\n",
      "\n",
      "\n",
      "loss for fold 12: \n",
      " Regressor 1: 0.6798778560000187\n",
      " Regressor 2: 0.3945196799999984\n",
      " Regressor 3: 9.149226560000002\n",
      "\n",
      "\n",
      "loss for fold 13: \n",
      " Regressor 1: 0.37434855605994954\n",
      " Regressor 2: 0.21384718859982144\n",
      " Regressor 3: 9.787676971400181\n",
      "\n",
      "\n",
      "loss for fold 14: \n",
      " Regressor 1: 0.6638120881198901\n",
      " Regressor 2: 0.4093833371998272\n",
      " Regressor 3: 10.019183382800176\n",
      "\n",
      "\n",
      "loss for fold 15: \n",
      " Regressor 1: 0.5520344282997769\n",
      " Regressor 2: 0.3467821829999451\n",
      " Regressor 3: 10.483897817000056\n",
      "\n",
      "\n",
      "loss for fold 16: \n",
      " Regressor 1: 0.7752729952202628\n",
      " Regressor 2: 0.5659129482000935\n",
      " Regressor 3: 11.779583028200094\n",
      "\n",
      "\n",
      "loss for fold 17: \n",
      " Regressor 1: 0.1723344631800856\n",
      " Regressor 2: 0.12088595579988493\n",
      " Regressor 3: 11.462457084200114\n",
      "\n",
      "\n",
      "loss for fold 18: \n",
      " Regressor 1: 0.4062786414602684\n",
      " Regressor 2: 0.1938664026001753\n",
      " Regressor 3: 12.139371362600176\n",
      "\n",
      "\n",
      "loss for fold 19: \n",
      " Regressor 1: 0.21896734253984818\n",
      " Regressor 2: 0.2905438373998841\n",
      " Regressor 3: 12.015418082600114\n",
      "\n",
      "\n",
      "loss for fold 20: \n",
      " Regressor 1: 0.056760218039791255\n",
      " Regressor 2: 0.02629113240000791\n",
      " Regressor 3: 12.644228867599988\n",
      "\n",
      "\n",
      "loss for fold 21: \n",
      " Regressor 1: 0.2535331147998008\n",
      " Regressor 2: 0.16904555632000395\n",
      " Regressor 3: 12.875939723679995\n",
      "\n",
      "\n",
      "loss for fold 22: \n",
      " Regressor 1: 0.40166598407977006\n",
      " Regressor 2: 0.301960997279906\n",
      " Regressor 3: 13.133202842720092\n",
      "\n",
      "\n",
      "loss for fold 23: \n",
      " Regressor 1: 0.4562119840798786\n",
      " Regressor 2: 0.2958574747200622\n",
      " Regressor 3: 13.55100428527994\n",
      "\n",
      "\n",
      "loss for fold 24: \n",
      " Regressor 1: 0.010907418320250883\n",
      " Regressor 2: 0.4907925428801114\n",
      " Regressor 3: 14.776677662880111\n",
      "\n",
      "\n",
      "loss for fold 25: \n",
      " Regressor 1: 2.377510048820241\n",
      " Regressor 2: 0.48205401212005583\n",
      " Regressor 3: 15.240094012120057\n",
      "\n",
      "\n",
      "loss for fold 26: \n",
      " Regressor 1: 2.8796481141600268\n",
      " Regressor 2: 0.04116045743990426\n",
      " Regressor 3: 15.227972022560099\n",
      "\n",
      "\n",
      "loss for fold 27: \n",
      " Regressor 1: 0.8594870456801686\n",
      " Regressor 2: 0.44307049712007895\n",
      " Regressor 3: 15.381898142879919\n",
      "\n",
      "\n",
      "loss for fold 28: \n",
      " Regressor 1: 0.3474741640198644\n",
      " Regressor 2: 0.36444404867992475\n",
      " Regressor 3: 16.066910511320074\n",
      "\n",
      "\n",
      "loss for fold 29: \n",
      " Regressor 1: 0.28078221067960385\n",
      " Regressor 2: 0.30899341912010136\n",
      " Regressor 3: 16.7851029008799\n",
      "\n",
      "\n",
      "loss for fold 30: \n",
      " Regressor 1: 0.06234888370009628\n",
      " Regressor 2: 0.12716172412004667\n",
      " Regressor 3: 17.946161724120046\n",
      "\n",
      "\n",
      "loss for fold 31: \n",
      " Regressor 1: 0.33894330361992075\n",
      " Regressor 2: 0.16395286124008734\n",
      " Regressor 3: 18.44791881875991\n",
      "\n",
      "\n",
      "loss for fold 32: \n",
      " Regressor 1: 2.4284946410797943\n",
      " Regressor 2: 0.15022137635996557\n",
      " Regressor 3: 19.628738816359967\n",
      "\n",
      "\n",
      "loss for fold 33: \n",
      " Regressor 1: 3.211186618679527\n",
      " Regressor 2: 0.013774723559922819\n",
      " Regressor 3: 20.438518083559924\n",
      "\n",
      "\n",
      "loss for fold 34: \n",
      " Regressor 1: 0.21660425575950626\n",
      " Regressor 2: 0.3090980800799823\n",
      " Regressor 3: 21.14725743992002\n",
      "\n",
      "\n",
      "loss for fold 35: \n",
      " Regressor 1: 0.10825064088009384\n",
      " Regressor 2: 0.31944037896002975\n",
      " Regressor 3: 22.259719621039974\n",
      "\n",
      "\n",
      "loss for fold 36: \n",
      " Regressor 1: 0.11685849599997766\n",
      " Regressor 2: 0.39285503999997573\n",
      " Regressor 3: 24.191817919999977\n",
      "\n",
      "\n",
      "loss for fold 37: \n",
      " Regressor 1: 0.16368105600032123\n",
      " Regressor 2: 0.40450752000002144\n",
      " Regressor 3: 24.71706271999998\n",
      "\n",
      "\n",
      "loss for fold 38: \n",
      " Regressor 1: 0.15872179199993752\n",
      " Regressor 2: 0.466099200000035\n",
      " Regressor 3: 26.086688959999965\n",
      "\n",
      "\n",
      "loss for fold 39: \n",
      " Regressor 1: 0.022192799999558588\n",
      " Regressor 2: 0.25136063999994906\n",
      " Regressor 3: 28.349783359999947\n",
      "\n",
      "\n",
      "loss for fold 40: \n",
      " Regressor 1: 0.09821971199985313\n",
      " Regressor 2: 0.40284288000004853\n",
      " Regressor 3: 29.36143711999995\n",
      "\n",
      "\n",
      "loss for fold 41: \n",
      " Regressor 1: 0.08350972800058543\n",
      " Regressor 2: 0.46110528000003104\n",
      " Regressor 3: 31.09506079999997\n",
      "\n",
      "\n",
      "loss for fold 42: \n",
      " Regressor 1: 0.05860271999951294\n",
      " Regressor 2: 0.4477881600000324\n",
      " Regressor 3: 33.032098879999964\n",
      "\n",
      "\n",
      "loss for fold 43: \n",
      " Regressor 1: 0.0417351840001885\n",
      " Regressor 2: 0.5734850399999818\n",
      " Regressor 3: 34.96776392000002\n",
      "\n",
      "\n",
      "loss for fold 44: \n",
      " Regressor 1: 0.0658163520001196\n",
      " Regressor 2: 0.5242622400000063\n",
      " Regressor 3: 38.27032016000001\n",
      "\n",
      "\n",
      "loss for fold 45: \n",
      " Regressor 1: 0.0053567790596531495\n",
      " Regressor 2: 0.36313537567000864\n",
      " Regressor 3: 39.73698462432999\n",
      "\n",
      "\n",
      "loss for fold 46: \n",
      " Regressor 1: 0.15378380328006358\n",
      " Regressor 2: 0.22653522803999937\n",
      " Regressor 3: 42.38270605196\n",
      "\n",
      "\n",
      "loss for fold 47: \n",
      " Regressor 1: 0.42385112679985326\n",
      " Regressor 2: 1.609103159700073\n",
      " Regressor 3: 0.2908968402999257\n",
      "\n",
      "\n",
      "loss for fold 48: \n",
      " Regressor 1: 1.2155364160003046\n",
      " Regressor 2: 0.5858396159999053\n",
      " Regressor 3: 0.5858396159999053\n",
      "\n",
      "\n",
      "loss for fold 49: \n",
      " Regressor 1: 3.3271044959997838\n",
      " Regressor 2: 0.11391158400002155\n",
      " Regressor 3: 4.286088415999981\n",
      "\n",
      "\n",
      "loss for fold 50: \n",
      " Regressor 1: 3.303814792000253\n",
      " Regressor 2: 0.8923046079999555\n",
      " Regressor 3: 3.4076953920000443\n",
      "\n",
      "\n",
      "loss for fold 51: \n",
      " Regressor 1: 1.1434652177602516\n",
      " Regressor 2: 1.176832912000012\n",
      " Regressor 3: 0.6231670879999882\n",
      "\n",
      "\n",
      "loss for fold 52: \n",
      " Regressor 1: 0.5904791429599641\n",
      " Regressor 2: 0.45275418477990925\n",
      " Regressor 3: 1.0540174152200876\n",
      "\n",
      "\n",
      "loss for fold 53: \n",
      " Regressor 1: 0.3923021967999816\n",
      " Regressor 2: 0.22063739776005065\n",
      " Regressor 3: 0.6922794022399508\n",
      "\n",
      "\n",
      "loss for fold 54: \n",
      " Regressor 1: 0.26910239199973063\n",
      " Regressor 2: 0.0014295551998984024\n",
      " Regressor 3: 0.387670755199899\n",
      "\n",
      "\n",
      "loss for fold 55: \n",
      " Regressor 1: 0.20545543680031386\n",
      " Regressor 2: 0.18991291584011805\n",
      " Regressor 3: 0.26636251584011816\n",
      "\n",
      "\n",
      "loss for fold 56: \n",
      " Regressor 1: 0.03441406079971898\n",
      " Regressor 2: 6.882623996062875e-05\n",
      " Regressor 3: 0.4782811737600383\n",
      "\n",
      "\n",
      "loss for fold 57: \n",
      " Regressor 1: 0.2247506128001433\n",
      " Regressor 2: 0.1648107526400402\n",
      " Regressor 3: 0.9874651526400413\n",
      "\n",
      "\n",
      "loss for fold 58: \n",
      " Regressor 1: 0.47764072320016737\n",
      " Regressor 2: 0.30366583615997556\n",
      " Regressor 3: 1.4162230361599768\n",
      "\n",
      "\n",
      "loss for fold 59: \n",
      " Regressor 1: 0.5380976816000071\n",
      " Regressor 2: 0.3163315244797893\n",
      " Regressor 3: 1.6675843244797868\n",
      "\n",
      "\n",
      "loss for fold 60: \n",
      " Regressor 1: 0.525881712399709\n",
      " Regressor 2: 0.32995594573333165\n",
      " Regressor 3: 1.8718915457333338\n",
      "\n",
      "\n",
      "loss for fold 61: \n",
      " Regressor 1: 0.8239126127999086\n",
      " Regressor 2: 0.49065898624010273\n",
      " Regressor 3: 2.178458986240102\n",
      "\n",
      "\n",
      "loss for fold 62: \n",
      " Regressor 1: 1.2130340128000725\n",
      " Regressor 2: 0.6342374630399377\n",
      " Regressor 3: 2.426277863039939\n",
      "\n",
      "\n",
      "loss for fold 63: \n",
      " Regressor 1: 0.9213765872001076\n",
      " Regressor 2: 0.48303329855997745\n",
      " Regressor 3: 2.34088449855998\n",
      "\n",
      "\n",
      "loss for fold 64: \n",
      " Regressor 1: 0.4458206975997214\n",
      " Regressor 2: 0.257222890879909\n",
      " Regressor 3: 2.1456496908799068\n",
      "\n",
      "\n",
      "loss for fold 65: \n",
      " Regressor 1: 0.943214843199712\n",
      " Regressor 2: 0.5344243929598811\n",
      " Regressor 3: 2.4213859929598804\n",
      "\n",
      "\n",
      "loss for fold 66: \n",
      " Regressor 1: 0.26137846719993263\n",
      " Regressor 2: 0.15896400255996923\n",
      " Regressor 3: 2.0156140025599676\n",
      "\n",
      "\n",
      "loss for fold 67: \n",
      " Regressor 1: 0.8198029535998899\n",
      " Regressor 2: 0.4588774860798672\n",
      " Regressor 3: 2.2595638860798672\n",
      "\n",
      "\n",
      "loss for fold 68: \n",
      " Regressor 1: 0.09845449279988827\n",
      " Regressor 2: 0.07932945343993225\n",
      " Regressor 3: 1.8015946534399312\n",
      "\n",
      "\n",
      "loss for fold 69: \n",
      " Regressor 1: 0.7340514415999451\n",
      " Regressor 2: 0.42082285887994075\n",
      " Regressor 3: 2.045403658879941\n",
      "\n",
      "\n",
      "loss for fold 70: \n",
      " Regressor 1: 0.24043904800032578\n",
      " Regressor 2: 0.09545300800004242\n",
      " Regressor 3: 1.4153745919999565\n",
      "\n",
      "\n",
      "loss for fold 71: \n",
      " Regressor 1: 0.2939890512002066\n",
      " Regressor 2: 0.11050760256000203\n",
      " Regressor 3: 1.2736923974399978\n",
      "\n",
      "\n",
      "loss for fold 72: \n",
      " Regressor 1: 0.3701631359997677\n",
      " Regressor 2: 0.31443705600005956\n",
      " Regressor 3: 1.5623294560000573\n",
      "\n",
      "\n",
      "loss for fold 73: \n",
      " Regressor 1: 1.768271232000437\n",
      " Regressor 2: 0.24949228800014112\n",
      " Regressor 3: 1.354591488000139\n",
      "\n",
      "\n",
      "loss for fold 74: \n",
      " Regressor 1: 2.073176742400207\n",
      " Regressor 2: 0.10598116608008112\n",
      " Regressor 3: 1.0649959660800796\n",
      "\n",
      "\n",
      "loss for fold 75: \n",
      " Regressor 1: 0.861769684800386\n",
      " Regressor 2: 0.2774061478400114\n",
      " Regressor 3: 0.5354274521599862\n",
      "\n",
      "\n",
      "loss for fold 76: \n",
      " Regressor 1: 0.6528374491395539\n",
      " Regressor 2: 0.3078044411201155\n",
      " Regressor 3: 0.361945558879885\n",
      "\n",
      "\n",
      "loss for fold 77: \n",
      " Regressor 1: 0.1414662173401382\n",
      " Regressor 2: 0.04536845192006567\n",
      " Regressor 3: 0.5783268519200675\n",
      "\n",
      "\n",
      "loss for fold 78: \n",
      " Regressor 1: 2.0593783060403155\n",
      " Regressor 2: 1.3248829711999903\n",
      " Regressor 3: 0.9192297711999906\n",
      "\n",
      "\n",
      "loss for fold 79: \n",
      " Regressor 1: 0.26113549570037264\n",
      " Regressor 2: 0.027787204720093328\n",
      " Regressor 3: 0.31881600472009275\n",
      "\n",
      "\n",
      "loss for fold 80: \n",
      " Regressor 1: 1.1786803204598186\n",
      " Regressor 2: 0.5702581041600254\n",
      " Regressor 3: 0.7625377041600245\n",
      "\n",
      "\n",
      "loss for fold 81: \n",
      " Regressor 1: 0.16804393440027354\n",
      " Regressor 2: 0.07206872192000624\n",
      " Regressor 3: 0.18466872192000672\n",
      "\n",
      "\n",
      "loss for fold 82: \n",
      " Regressor 1: 0.792563305600126\n",
      " Regressor 2: 0.37568712448002906\n",
      " Regressor 3: 0.43087152448002897\n",
      "\n",
      "\n",
      "loss for fold 83: \n",
      " Regressor 1: 0.4940006384001663\n",
      " Regressor 2: 0.2620611795200478\n",
      " Regressor 3: 0.2388339795200487\n",
      "\n",
      "\n",
      "loss for fold 84: \n",
      " Regressor 1: 0.9848995599998176\n",
      " Regressor 2: 0.27410023999995126\n",
      " Regressor 3: 0.29402303999995105\n",
      "\n",
      "\n",
      "loss for fold 85: \n",
      " Regressor 1: 2.4510802959994464\n",
      " Regressor 2: 0.4083543040000528\n",
      " Regressor 3: 0.3598887040000527\n",
      "\n",
      "\n",
      "loss for fold 86: \n",
      " Regressor 1: 1.1966602079995141\n",
      " Regressor 2: 0.9989353920000532\n",
      " Regressor 3: 0.8868853920000532\n",
      "\n",
      "\n",
      "loss for fold 87: \n",
      " Regressor 1: 0.6514015039999492\n",
      " Regressor 2: 0.4870263040000129\n",
      " Regressor 3: 0.2731559040000118\n",
      "\n",
      "\n",
      "loss for fold 88: \n",
      " Regressor 1: 0.9640364559997465\n",
      " Regressor 2: 0.5023844960000297\n",
      " Regressor 3: 0.14526329600002974\n",
      "\n",
      "\n",
      "loss for fold 89: \n",
      " Regressor 1: 1.4832795999991788\n",
      " Regressor 2: 0.7162812799999614\n",
      " Regressor 3: 0.17128447999996155\n",
      "\n",
      "\n",
      "loss for fold 90: \n",
      " Regressor 1: 1.6186863439997552\n",
      " Regressor 2: 0.779383264000046\n",
      " Regressor 3: 0.001308335999954835\n",
      "\n",
      "\n",
      "loss for fold 91: \n",
      " Regressor 1: 2.1023932159997827\n",
      " Regressor 2: 1.0090548160000452\n",
      " Regressor 3: 0.05834518399995403\n",
      "\n",
      "\n",
      "loss for fold 92: \n",
      " Regressor 1: 1.8925227199994623\n",
      " Regressor 2: 0.9057188800000446\n",
      " Regressor 3: 0.5025975199999557\n",
      "\n",
      "\n",
      "loss for fold 93: \n",
      " Regressor 1: 1.1619237440002763\n",
      " Regressor 2: 0.5534215839999987\n",
      " Regressor 3: 1.253213616000001\n",
      "\n",
      "\n",
      "loss for fold 94: \n",
      " Regressor 1: 1.2221646400002015\n",
      " Regressor 2: 0.575401039999992\n",
      " Regressor 3: 1.6901497600000086\n",
      "\n",
      "\n",
      "loss for fold 95: \n",
      " Regressor 1: 0.4522172239996536\n",
      " Regressor 2: 0.2589773439999892\n",
      " Regressor 3: 3.0472349439999893\n",
      "\n",
      "\n",
      "loss for fold 96: \n",
      " Regressor 1: 1.1606818079995094\n",
      " Regressor 2: 0.1439069280000096\n",
      " Regressor 3: 3.23404307199999\n",
      "\n",
      "\n",
      "loss for fold 97: \n",
      " Regressor 1: 1.0390856277600378\n",
      " Regressor 2: 1.3340277336798678\n",
      " Regressor 3: 22.056820316320135\n",
      "\n",
      "\n",
      "loss for fold 98: \n",
      " Regressor 1: 1.2693771671204956\n",
      " Regressor 2: 1.26610081816008\n",
      " Regressor 3: 6.740262571839921\n",
      "\n",
      "\n",
      "loss for fold 99: \n",
      " Regressor 1: 7.799143983507221\n",
      " Regressor 2: 5.472878142976802\n",
      " Regressor 3: 10.182644782976801\n",
      "\n",
      "\n",
      "loss for fold 100: \n",
      " Regressor 1: 1.4038716379195435\n",
      " Regressor 2: 0.31400239943993746\n",
      " Regressor 3: 8.163217700560061\n",
      "\n",
      "\n",
      "loss for fold 101: \n",
      " Regressor 1: 0.5409711333597258\n",
      " Regressor 2: 0.6234897886398798\n",
      " Regressor 3: 21.87917417136012\n",
      "\n",
      "\n",
      "loss for fold 102: \n",
      " Regressor 1: 0.3684118506598679\n",
      " Regressor 2: 0.44444709804012206\n",
      " Regressor 3: 21.983878638040125\n",
      "\n",
      "\n",
      "loss for fold 103: \n",
      " Regressor 1: 0.418814604059655\n",
      " Regressor 2: 0.5136690545997951\n",
      " Regressor 3: 20.060817205400205\n",
      "\n",
      "\n",
      "loss for fold 104: \n",
      " Regressor 1: 0.35412060689977665\n",
      " Regressor 2: 0.44076862004019546\n",
      " Regressor 3: 19.168043049959806\n",
      "\n",
      "\n",
      "loss for fold 105: \n",
      " Regressor 1: 0.33876615360024687\n",
      " Regressor 2: 0.411841204800254\n",
      " Regressor 3: 19.05523250480025\n",
      "\n",
      "\n",
      "loss for fold 106: \n",
      " Regressor 1: 0.29839027007998453\n",
      " Regressor 2: 0.3820851434400687\n",
      " Regressor 3: 17.29712353655993\n",
      "\n",
      "\n",
      "loss for fold 107: \n",
      " Regressor 1: 0.365218074240083\n",
      " Regressor 2: 0.455595532320217\n",
      " Regressor 3: 17.172842872320217\n",
      "\n",
      "\n",
      "loss for fold 108: \n",
      " Regressor 1: 0.33003963648001644\n",
      " Regressor 2: 0.4165497986399487\n",
      " Regressor 3: 16.175040628639948\n",
      "\n",
      "\n",
      "loss for fold 109: \n",
      " Regressor 1: 0.3036825926401079\n",
      " Regressor 2: 0.3880452235201233\n",
      " Regressor 3: 15.191967893520125\n",
      "\n",
      "\n",
      "loss for fold 110: \n",
      " Regressor 1: 0.16059656832014468\n",
      " Regressor 2: 0.20451205776001657\n",
      " Regressor 3: 14.059038457760018\n",
      "\n",
      "\n",
      "loss for fold 111: \n",
      " Regressor 1: 0.3051716640004116\n",
      " Regressor 2: 0.40317130199973406\n",
      " Regressor 3: 13.314456851999736\n",
      "\n",
      "\n",
      "loss for fold 112: \n",
      " Regressor 1: 0.2556434590597334\n",
      " Regressor 2: 0.3413568259200339\n",
      " Regressor 3: 12.316540495920036\n",
      "\n",
      "\n",
      "loss for fold 113: \n",
      " Regressor 1: 0.27376221092030306\n",
      " Regressor 2: 0.37190404223976614\n",
      " Regressor 3: 11.419108312239764\n",
      "\n",
      "\n",
      "loss for fold 114: \n",
      " Regressor 1: 0.2604976082596835\n",
      " Regressor 2: 0.35845694688002894\n",
      " Regressor 3: 10.486787856880028\n",
      "\n",
      "\n",
      "loss for fold 115: \n",
      " Regressor 1: 0.02655431486022053\n",
      " Regressor 2: 0.021059710239846652\n",
      " Regressor 3: 9.240606810239846\n",
      "\n",
      "\n",
      "loss for fold 116: \n",
      " Regressor 1: 0.2516076063605084\n",
      " Regressor 2: 0.6003253640001986\n",
      " Regressor 3: 7.721511025999803\n",
      "\n",
      "\n",
      "loss for fold 117: \n",
      " Regressor 1: 0.08827645795948769\n",
      " Regressor 2: 0.11201513824015663\n",
      " Regressor 3: 7.324167171759846\n",
      "\n",
      "\n",
      "loss for fold 118: \n",
      " Regressor 1: 0.2094742854395122\n",
      " Regressor 2: 0.3010858527998259\n",
      " Regressor 3: 6.864654252799827\n",
      "\n",
      "\n",
      "loss for fold 119: \n",
      " Regressor 1: 0.09935721614046145\n",
      " Regressor 2: 0.17544840767986614\n",
      " Regressor 3: 5.529529772320135\n",
      "\n",
      "\n",
      "loss for fold 120: \n",
      " Regressor 1: 0.22316186097972945\n",
      " Regressor 2: 0.3620873284798094\n",
      " Regressor 3: 5.22348252847981\n",
      "\n",
      "\n",
      "loss for fold 121: \n",
      " Regressor 1: 0.19277210687982205\n",
      " Regressor 2: 0.3207291758400892\n",
      " Regressor 3: 4.354532165840087\n",
      "\n",
      "\n",
      "loss for fold 122: \n",
      " Regressor 1: 0.07115979648018111\n",
      " Regressor 2: 0.14791992863995063\n",
      " Regressor 3: 3.0752651513600497\n",
      "\n",
      "\n",
      "loss for fold 123: \n",
      " Regressor 1: 0.148154728319426\n",
      " Regressor 2: 0.30161743775994054\n",
      " Regressor 3: 2.128907562240059\n",
      "\n",
      "\n",
      "loss for fold 124: \n",
      " Regressor 1: 0.13208030015983496\n",
      " Regressor 2: 0.24776846688007126\n",
      " Regressor 3: 1.9045747668800708\n",
      "\n",
      "\n",
      "loss for fold 125: \n",
      " Regressor 1: 0.11408089152022427\n",
      " Regressor 2: 0.2652566553599556\n",
      " Regressor 3: 0.6377558446400435\n",
      "\n",
      "\n",
      "loss for fold 126: \n",
      " Regressor 1: 0.13240229760004496\n",
      " Regressor 2: 0.2783713968000505\n",
      " Regressor 3: 0.4484985368000487\n",
      "\n",
      "\n",
      "loss for fold 127: \n",
      " Regressor 1: 0.10167805439951394\n",
      " Regressor 2: 0.22338448920007892\n",
      " Regressor 3: 0.3174817507999208\n",
      "\n",
      "\n",
      "loss for fold 128: \n",
      " Regressor 1: 0.08177088960062662\n",
      " Regressor 2: 0.25152110280008344\n",
      " Regressor 3: 1.4805052228000832\n",
      "\n",
      "\n",
      "loss for fold 129: \n",
      " Regressor 1: 0.04827664896054529\n",
      " Regressor 2: 0.1785852652801303\n",
      " Regressor 3: 2.0718282152801315\n",
      "\n",
      "\n",
      "loss for fold 130: \n",
      " Regressor 1: 0.08392059647938765\n",
      " Regressor 2: 0.2459180786400097\n",
      " Regressor 3: 2.286741121359988\n",
      "\n",
      "\n",
      "loss for fold 131: \n",
      " Regressor 1: 0.011081894834354955\n",
      " Regressor 2: 0.1522263876883656\n",
      " Regressor 3: 2.9940229523116315\n",
      "\n",
      "\n",
      "loss for fold 132: \n",
      " Regressor 1: 0.7257679242670108\n",
      " Regressor 2: 0.42910639435472575\n",
      " Regressor 3: 4.162136234354726\n",
      "\n",
      "\n",
      "loss for fold 133: \n",
      " Regressor 1: 6.123964051359295\n",
      " Regressor 2: 4.319382885031713\n",
      " Regressor 3: 8.611400035031712\n",
      "\n",
      "\n",
      "loss for fold 134: \n",
      " Regressor 1: 6.123033067999302\n",
      " Regressor 2: 4.033436330551716\n",
      " Regressor 3: 8.855664080551717\n",
      "\n",
      "\n",
      "loss for fold 135: \n",
      " Regressor 1: 0.7383881140361837\n",
      " Regressor 2: 0.3389405787536095\n",
      " Regressor 3: 5.661618678753609\n",
      "\n",
      "\n",
      "loss for fold 136: \n",
      " Regressor 1: 0.07965935388837408\n",
      " Regressor 2: 0.12046091166113015\n",
      " Regressor 3: 5.67192375833887\n",
      "\n",
      "\n",
      "loss for fold 137: \n",
      " Regressor 1: 0.015613477180032476\n",
      " Regressor 2: 0.16290630536590278\n",
      " Regressor 3: 6.067457604634097\n",
      "\n",
      "\n",
      "loss for fold 138: \n",
      " Regressor 1: 0.02614534150004033\n",
      " Regressor 2: 0.14677826960610396\n",
      " Regressor 3: 6.488854040393894\n",
      "\n",
      "\n",
      "loss for fold 139: \n",
      " Regressor 1: 0.03378862271969041\n",
      " Regressor 2: 0.03063470304011595\n",
      " Regressor 3: 7.037841013040117\n",
      "\n",
      "\n",
      "loss for fold 140: \n",
      " Regressor 1: 0.02913778560026259\n",
      " Regressor 2: 0.11257786920002033\n",
      " Regressor 3: 7.23152453079998\n",
      "\n",
      "\n",
      "loss for fold 141: \n",
      " Regressor 1: 0.052501272960171264\n",
      " Regressor 2: 0.12526390271989563\n",
      " Regressor 3: 7.520073127280105\n",
      "\n",
      "\n",
      "loss for fold 142: \n",
      " Regressor 1: 0.05738460288018388\n",
      " Regressor 2: 0.107483246159914\n",
      " Regressor 3: 7.802443423840087\n",
      "\n",
      "\n",
      "loss for fold 143: \n",
      " Regressor 1: 0.11118729408021721\n",
      " Regressor 2: 0.07997382456001212\n",
      " Regressor 3: 8.216861604560012\n",
      "\n",
      "\n",
      "loss for fold 144: \n",
      " Regressor 1: 0.11751759936053219\n",
      " Regressor 2: 0.06652471751992906\n",
      " Regressor 3: 8.39176155751993\n",
      "\n",
      "\n",
      "loss for fold 145: \n",
      " Regressor 1: 0.1292343455999685\n",
      " Regressor 2: 0.1053160391999306\n",
      " Regressor 3: 8.368674260800068\n",
      "\n",
      "\n",
      "loss for fold 146: \n",
      " Regressor 1: 0.318051878759519\n",
      " Regressor 2: 0.19902202902007105\n",
      " Regressor 3: 8.383142600979928\n",
      "\n",
      "\n",
      "loss for fold 147: \n",
      " Regressor 1: 2.5882615599998147\n",
      " Regressor 2: 0.6388151460000948\n",
      " Regressor 3: 12.919226196000096\n",
      "\n",
      "\n",
      "loss for fold 148: \n",
      " Regressor 1: 1.5485397324001369\n",
      " Regressor 2: 0.5215223399199118\n",
      " Regressor 3: 6.715301030080088\n",
      "\n",
      "\n",
      "loss for fold 149: \n",
      " Regressor 1: 0.4034393955995057\n",
      " Regressor 2: 0.9454005700799915\n",
      " Regressor 3: 5.769690570079991\n",
      "\n",
      "\n",
      "loss for fold 150: \n",
      " Regressor 1: 5.046379809591983\n",
      " Regressor 2: 4.176944231266965\n",
      " Regressor 3: 5.033817152733034\n",
      "\n",
      "\n",
      "loss for fold 151: \n",
      " Regressor 1: 3.20047307868019\n",
      " Regressor 2: 1.6581938978999977\n",
      " Regressor 3: 14.767642322100002\n",
      "\n",
      "\n",
      "loss for fold 152: \n",
      " Regressor 1: 0.33247551688001664\n",
      " Regressor 2: 0.006620183299911986\n",
      " Regressor 3: 11.491835523299908\n",
      "\n",
      "\n",
      "loss for fold 153: \n",
      " Regressor 1: 0.29390640312006866\n",
      " Regressor 2: 0.07386277121987206\n",
      " Regressor 3: 10.66718701878013\n",
      "\n",
      "\n",
      "loss for fold 154: \n",
      " Regressor 1: 0.35897902088011335\n",
      " Regressor 2: 0.11661442190005644\n",
      " Regressor 3: 9.939371548099942\n",
      "\n",
      "\n",
      "loss for fold 155: \n",
      " Regressor 1: 0.4920301716799642\n",
      " Regressor 2: 0.18648125059988274\n",
      " Regressor 3: 9.242025249400115\n",
      "\n",
      "\n",
      "loss for fold 156: \n",
      " Regressor 1: 0.036824068320306935\n",
      " Regressor 2: 0.019696444200057783\n",
      " Regressor 3: 8.837397555799942\n",
      "\n",
      "\n",
      "loss for fold 157: \n",
      " Regressor 1: 1.1925178607998177\n",
      " Regressor 2: 0.4322127110399734\n",
      " Regressor 3: 7.908018388960027\n",
      "\n",
      "\n",
      "loss for fold 158: \n",
      " Regressor 1: 1.432388475649752\n",
      " Regressor 2: 0.5039967688700138\n",
      " Regressor 3: 7.372403661129987\n",
      "\n",
      "\n",
      "loss for fold 159: \n",
      " Regressor 1: 1.4197148379495488\n",
      " Regressor 2: 0.5010868267567545\n",
      " Regressor 3: 6.962997783243246\n",
      "\n",
      "\n",
      "loss for fold 160: \n",
      " Regressor 1: 1.2040785268746816\n",
      " Regressor 2: 0.4382590026783042\n",
      " Regressor 3: 6.663507257321694\n",
      "\n",
      "\n",
      "loss for fold 161: \n",
      " Regressor 1: 0.20547525929996624\n",
      " Regressor 2: 0.03234074109995433\n",
      " Regressor 3: 6.820268741099955\n",
      "\n",
      "\n",
      "loss for fold 162: \n",
      " Regressor 1: 0.2519793166450697\n",
      " Regressor 2: 0.06753220075633593\n",
      " Regressor 3: 6.588584660756336\n",
      "\n",
      "\n",
      "loss for fold 163: \n",
      " Regressor 1: 0.8114938814551422\n",
      " Regressor 2: 0.3529531595170283\n",
      " Regressor 3: 5.946669110482972\n",
      "\n",
      "\n",
      "loss for fold 164: \n",
      " Regressor 1: 1.1762156870099503\n",
      " Regressor 2: 0.5098431907473149\n",
      " Regressor 3: 5.612276859252685\n",
      "\n",
      "\n",
      "loss for fold 165: \n",
      " Regressor 1: 1.17918234495\n",
      " Regressor 2: 0.5029992164500854\n",
      " Regressor 3: 5.4840292035499125\n",
      "\n",
      "\n",
      "loss for fold 166: \n",
      " Regressor 1: 1.3662977541002306\n",
      " Regressor 2: 0.5769676610199035\n",
      " Regressor 3: 5.3158623389800965\n",
      "\n",
      "\n",
      "loss for fold 167: \n",
      " Regressor 1: 1.3997514751999702\n",
      " Regressor 2: 0.596794495360065\n",
      " Regressor 3: 5.241212924639935\n",
      "\n",
      "\n",
      "loss for fold 168: \n",
      " Regressor 1: 1.179202753599526\n",
      " Regressor 2: 0.5150748684798678\n",
      " Regressor 3: 5.3059684415201325\n",
      "\n",
      "\n",
      "loss for fold 169: \n",
      " Regressor 1: 0.3693026097666703\n",
      " Regressor 2: 0.16119152803991454\n",
      " Regressor 3: 5.679228761960086\n",
      "\n",
      "\n",
      "loss for fold 170: \n",
      " Regressor 1: 0.7720949714256449\n",
      " Regressor 2: 0.45637335085506336\n",
      " Regressor 3: 5.438247629144936\n",
      "\n",
      "\n",
      "loss for fold 171: \n",
      " Regressor 1: 0.6204881759997889\n",
      " Regressor 2: 0.7749890080000004\n",
      " Regressor 3: 5.207138991999999\n",
      "\n",
      "\n",
      "loss for fold 172: \n",
      " Regressor 1: 0.025113524916392294\n",
      " Regressor 2: 0.10924742873996074\n",
      " Regressor 3: 5.992176551260039\n",
      "\n",
      "\n",
      "loss for fold 173: \n",
      " Regressor 1: 0.657815496377097\n",
      " Regressor 2: 0.37949448296397925\n",
      " Regressor 3: 5.871497067036023\n",
      "\n",
      "\n",
      "loss for fold 174: \n",
      " Regressor 1: 0.36340147330858485\n",
      " Regressor 2: 0.19728512769008155\n",
      " Regressor 3: 6.626598457690083\n",
      "\n",
      "\n",
      "loss for fold 175: \n",
      " Regressor 1: 0.1765550223995831\n",
      " Regressor 2: 0.10722688832007243\n",
      " Regressor 3: 6.742098828320074\n",
      "\n",
      "\n",
      "loss for fold 176: \n",
      " Regressor 1: 0.511175641599678\n",
      " Regressor 2: 0.27518751487992077\n",
      " Regressor 3: 6.59096248512008\n",
      "\n",
      "\n",
      "loss for fold 177: \n",
      " Regressor 1: 0.42579120153358474\n",
      " Regressor 2: 0.23609299877395173\n",
      " Regressor 3: 6.885537141226049\n",
      "\n",
      "\n",
      "loss for fold 178: \n",
      " Regressor 1: 0.4922089695813838\n",
      " Regressor 2: 0.2825097388147171\n",
      " Regressor 3: 7.117285251185283\n",
      "\n",
      "\n",
      "loss for fold 179: \n",
      " Regressor 1: 0.5828376075299815\n",
      " Regressor 2: 0.3357989034657525\n",
      " Regressor 3: 7.363328266534246\n",
      "\n",
      "\n",
      "loss for fold 180: \n",
      " Regressor 1: 1.5520092739064921\n",
      " Regressor 2: 0.8941070502525861\n",
      " Regressor 3: 7.124002249747415\n",
      "\n",
      "\n",
      "loss for fold 181: \n",
      " Regressor 1: 5.099115573116649\n",
      " Regressor 2: 2.9325706469779487\n",
      " Regressor 3: 5.422653353022053\n",
      "\n",
      "\n",
      "loss for fold 182: \n",
      " Regressor 1: 2.6602845617299806\n",
      " Regressor 2: 1.516530158273378\n",
      " Regressor 3: 7.192423741726621\n",
      "\n",
      "\n",
      "loss for fold 183: \n",
      " Regressor 1: 0.5161840793506371\n",
      " Regressor 2: 0.2807469302684389\n",
      " Regressor 3: 8.79703469973156\n",
      "\n",
      "\n",
      "loss for fold 184: \n",
      " Regressor 1: 0.6527151390788646\n",
      " Regressor 2: 0.43661875321872046\n",
      " Regressor 3: 9.023571056781279\n",
      "\n",
      "\n",
      "loss for fold 185: \n",
      " Regressor 1: 0.4518249120420421\n",
      " Regressor 2: 0.3202795655427799\n",
      " Regressor 3: 9.534381494457222\n",
      "\n",
      "\n",
      "loss for fold 186: \n",
      " Regressor 1: 0.4757158908103776\n",
      " Regressor 2: 0.36244427863805484\n",
      " Regressor 3: 9.897233721361946\n",
      "\n",
      "\n",
      "loss for fold 187: \n",
      " Regressor 1: 0.2951321744003508\n",
      " Regressor 2: 0.2525739667200204\n",
      " Regressor 3: 10.421149293279978\n",
      "\n",
      "\n",
      "loss for fold 188: \n",
      " Regressor 1: 0.3420117407997907\n",
      " Regressor 2: 0.3594623174400313\n",
      " Regressor 3: 10.73581715255997\n",
      "\n",
      "\n",
      "loss for fold 189: \n",
      " Regressor 1: 0.276981833599919\n",
      " Regressor 2: 0.13710703807999014\n",
      " Regressor 3: 11.65993628807999\n",
      "\n",
      "\n",
      "loss for fold 190: \n",
      " Regressor 1: 0.29687758239994366\n",
      " Regressor 2: 0.3469223283199776\n",
      " Regressor 3: 11.607932891680022\n",
      "\n",
      "\n",
      "loss for fold 191: \n",
      " Regressor 1: 0.2504166560002403\n",
      " Regressor 2: 0.26456992320000694\n",
      " Regressor 3: 12.125270076799993\n",
      "\n",
      "\n",
      "loss for fold 192: \n",
      " Regressor 1: 0.028369956799807028\n",
      " Regressor 2: 0.031247419040017377\n",
      " Regressor 3: 12.795018800959982\n",
      "\n",
      "\n",
      "loss for fold 193: \n",
      " Regressor 1: 0.2030572849598684\n",
      " Regressor 2: 0.14864309463999348\n",
      " Regressor 3: 13.411259604639993\n",
      "\n",
      "\n",
      "loss for fold 194: \n",
      " Regressor 1: 0.14839784239981668\n",
      " Regressor 2: 0.08180382167997768\n",
      " Regressor 3: 13.779177311679979\n",
      "\n",
      "\n",
      "loss for fold 195: \n",
      " Regressor 1: 0.36068327599998895\n",
      " Regressor 2: 0.07418686952001252\n",
      " Regressor 3: 14.203206649520013\n",
      "\n",
      "\n",
      "loss for fold 196: \n",
      " Regressor 1: 0.6653028728003676\n",
      " Regressor 2: 0.16907030943999724\n",
      " Regressor 3: 14.386967690560002\n",
      "\n",
      "\n",
      "loss for fold 197: \n",
      " Regressor 1: 2.8138314196596355\n",
      " Regressor 2: 0.7939661881499933\n",
      " Regressor 3: 14.799411811850005\n",
      "\n",
      "\n",
      "loss for fold 198: \n",
      " Regressor 1: 0.2116544919399388\n",
      " Regressor 2: 2.370248685859906\n",
      " Regressor 3: 8.769743894140092\n",
      "\n",
      "\n",
      "loss for fold 199: \n",
      " Regressor 1: 68.40863640499998\n",
      " Regressor 2: 11.826839554560067\n",
      " Regressor 3: 1.5752223004399326\n",
      "\n",
      "\n",
      "loss for fold 200: \n",
      " Regressor 1: 5.098998041800243\n",
      " Regressor 2: 1.6117621135601752\n",
      " Regressor 3: 7.385434163560177\n",
      "\n",
      "\n",
      "loss for fold 201: \n",
      " Regressor 1: 7.506067858999803\n",
      " Regressor 2: 0.12423430691986681\n",
      " Regressor 3: 6.700151493080131\n",
      "\n",
      "\n",
      "loss for fold 202: \n",
      " Regressor 1: 18.66291101759998\n",
      " Regressor 2: 0.41822131221983483\n",
      " Regressor 3: 8.390886462219836\n",
      "\n",
      "\n",
      "loss for fold 203: \n",
      " Regressor 1: 36.15299523579998\n",
      " Regressor 2: 0.5646599995800763\n",
      " Regressor 3: 6.152760510419924\n",
      "\n",
      "\n",
      "loss for fold 204: \n",
      " Regressor 1: 0.21672028572008628\n",
      " Regressor 2: 0.2591364970798651\n",
      " Regressor 3: 7.908627492920136\n",
      "\n",
      "\n",
      "loss for fold 205: \n",
      " Regressor 1: 0.44035358163978344\n",
      " Regressor 2: 0.545283856359795\n",
      " Regressor 3: 7.668722093640209\n",
      "\n",
      "\n",
      "loss for fold 206: \n",
      " Regressor 1: 3.292127037420098\n",
      " Regressor 2: 0.9793534209997148\n",
      " Regressor 3: 7.2693724690002846\n",
      "\n",
      "\n",
      "loss for fold 207: \n",
      " Regressor 1: 3.307685306619952\n",
      " Regressor 2: 0.8024247173798145\n",
      " Regressor 3: 7.469499082620185\n",
      "\n",
      "\n",
      "loss for fold 208: \n",
      " Regressor 1: 0.6318119560599449\n",
      " Regressor 2: 0.23185417354000037\n",
      " Regressor 3: 8.51545386354\n",
      "\n",
      "\n",
      "loss for fold 209: \n",
      " Regressor 1: 4.828407548159749\n",
      " Regressor 2: 0.13287004418007342\n",
      " Regressor 3: 8.150883505819923\n",
      "\n",
      "\n",
      "loss for fold 210: \n",
      " Regressor 1: 4.887949312419845\n",
      " Regressor 2: 0.284345304579972\n",
      " Regressor 3: 8.556730694579972\n",
      "\n",
      "\n",
      "loss for fold 211: \n",
      " Regressor 1: 0.6920223997201695\n",
      " Regressor 2: 0.36761746332017964\n",
      " Regressor 3: 8.617112663320182\n",
      "\n",
      "\n",
      "loss for fold 212: \n",
      " Regressor 1: 0.18978994387978076\n",
      " Regressor 2: 0.3648805081401676\n",
      " Regressor 3: 8.579963498140167\n",
      "\n",
      "\n",
      "loss for fold 213: \n",
      " Regressor 1: 0.07988616016001515\n",
      " Regressor 2: 0.31041411942017305\n",
      " Regressor 3: 8.479562869420171\n",
      "\n",
      "\n",
      "loss for fold 214: \n",
      " Regressor 1: 0.24846301187987763\n",
      " Regressor 2: 0.12721309454013152\n",
      " Regressor 3: 8.238905584540127\n",
      "\n",
      "\n",
      "loss for fold 215: \n",
      " Regressor 1: 1.4752412504399324\n",
      " Regressor 2: 0.18772859695984323\n",
      " Regressor 3: 8.230442796959846\n",
      "\n",
      "\n",
      "loss for fold 216: \n",
      " Regressor 1: 12.040590345360165\n",
      " Regressor 2: 0.18095050788008393\n",
      " Regressor 3: 7.781263382119917\n",
      "\n",
      "\n",
      "loss for fold 217: \n",
      " Regressor 1: 12.148446624360183\n",
      " Regressor 2: 0.12585477139982615\n",
      " Regressor 3: 7.996046321399824\n",
      "\n",
      "\n",
      "loss for fold 218: \n",
      " Regressor 1: 1.6167187338000382\n",
      " Regressor 2: 0.3199431605798395\n",
      " Regressor 3: 8.08659035057984\n",
      "\n",
      "\n",
      "loss for fold 219: \n",
      " Regressor 1: 0.26508074531989934\n",
      " Regressor 2: 0.3093439957200097\n",
      " Regressor 3: 7.960924795720011\n",
      "\n",
      "\n",
      "loss for fold 220: \n",
      " Regressor 1: 0.14705273778017514\n",
      " Regressor 2: 0.2771528492799398\n",
      " Regressor 3: 7.8021452392799375\n",
      "\n",
      "\n",
      "loss for fold 221: \n",
      " Regressor 1: 0.14266479779987762\n",
      " Regressor 2: 0.3331485586598717\n",
      " Regressor 3: 7.720030508659871\n",
      "\n",
      "\n",
      "loss for fold 222: \n",
      " Regressor 1: 0.1737606168000454\n",
      " Regressor 2: 0.3793242478801062\n",
      " Regressor 3: 7.616573737880106\n",
      "\n",
      "\n",
      "loss for fold 223: \n",
      " Regressor 1: 0.1594267757998118\n",
      " Regressor 2: 0.3309319734599825\n",
      " Regressor 3: 7.407026973459981\n",
      "\n",
      "\n",
      "loss for fold 224: \n",
      " Regressor 1: 0.18198495179979446\n",
      " Regressor 2: 0.3563163610398874\n",
      " Regressor 3: 7.259734851039891\n",
      "\n",
      "\n",
      "loss for fold 225: \n",
      " Regressor 1: 0.16633521719985112\n",
      " Regressor 2: 0.3106769588800411\n",
      " Regressor 3: 7.0298969088800405\n",
      "\n",
      "\n",
      "loss for fold 226: \n",
      " Regressor 1: 0.17831924819988743\n",
      " Regressor 2: 0.31553934952005847\n",
      " Regressor 3: 6.83903873952006\n",
      "\n",
      "\n",
      "loss for fold 227: \n",
      " Regressor 1: 0.24134726715987043\n",
      " Regressor 2: 0.1744068420399536\n",
      " Regressor 3: 6.4906636420399515\n",
      "\n",
      "\n",
      "loss for fold 228: \n",
      " Regressor 1: 2.8479210352401587\n",
      " Regressor 2: 0.32925074717991976\n",
      " Regressor 3: 5.768241442820081\n",
      "\n",
      "\n",
      "loss for fold 229: \n",
      " Regressor 1: 23.213797101640118\n",
      " Regressor 2: 0.17469069594003628\n",
      " Regressor 3: 6.041896245940038\n",
      "\n",
      "\n",
      "loss for fold 230: \n",
      " Regressor 1: 23.503152705040062\n",
      " Regressor 2: 0.5812799809200229\n",
      " Regressor 3: 6.20667687092002\n",
      "\n",
      "\n",
      "loss for fold 231: \n",
      " Regressor 1: 3.3805868561200043\n",
      " Regressor 2: 0.33077898559991326\n",
      " Regressor 3: 5.702845185599912\n",
      "\n",
      "\n",
      "loss for fold 232: \n",
      " Regressor 1: 0.728846303959994\n",
      " Regressor 2: 0.31177013673992704\n",
      " Regressor 3: 5.4189836267399265\n",
      "\n",
      "\n",
      "loss for fold 233: \n",
      " Regressor 1: 0.2399217157600475\n",
      " Regressor 2: 0.1372018853798771\n",
      " Regressor 3: 4.968040635379879\n",
      "\n",
      "\n",
      "loss for fold 234: \n",
      " Regressor 1: 0.29862952019998446\n",
      " Regressor 2: 0.3408914179599094\n",
      " Regressor 3: 4.202050572040092\n",
      "\n",
      "\n",
      "loss for fold 235: \n",
      " Regressor 1: 0.24580579139995606\n",
      " Regressor 2: 0.2988784288598758\n",
      " Regressor 3: 4.542401628859876\n",
      "\n",
      "\n",
      "loss for fold 236: \n",
      " Regressor 1: 0.25092837719996197\n",
      " Regressor 2: 0.29433272948003264\n",
      " Regressor 3: 4.226915119480033\n",
      "\n",
      "\n",
      "loss for fold 237: \n",
      " Regressor 1: 0.33218480699988007\n",
      " Regressor 2: 0.34419067994000585\n",
      " Regressor 3: 3.2659288700599944\n",
      "\n",
      "\n",
      "loss for fold 238: \n",
      " Regressor 1: 0.2518683011998988\n",
      " Regressor 2: 0.27615641485992626\n",
      " Regressor 3: 3.5522911048599255\n",
      "\n",
      "\n",
      "loss for fold 239: \n",
      " Regressor 1: 0.14712943679995405\n",
      " Regressor 2: 0.13527536537996454\n",
      " Regressor 3: 2.7953524346200354\n",
      "\n",
      "\n",
      "loss for fold 240: \n",
      " Regressor 1: 0.2672987201999817\n",
      " Regressor 2: 0.27540534355988555\n",
      " Regressor 3: 2.849004233559885\n",
      "\n",
      "\n",
      "loss for fold 241: \n",
      " Regressor 1: 0.23424472620005687\n",
      " Regressor 2: 0.23593575127998534\n",
      " Regressor 3: 2.440983701279986\n",
      "\n",
      "\n",
      "loss for fold 242: \n",
      " Regressor 1: 0.3388283658001079\n",
      " Regressor 2: 0.23769203425990248\n",
      " Regressor 3: 2.062667024259902\n",
      "\n",
      "\n",
      "loss for fold 243: \n",
      " Regressor 1: 0.7193572184800701\n",
      " Regressor 2: 0.1054051388799433\n",
      " Regressor 3: 1.538785138879943\n",
      "\n",
      "\n",
      "loss for fold 244: \n",
      " Regressor 1: 1.2506097109599974\n",
      " Regressor 2: 0.1699024268599043\n",
      " Regressor 3: 1.2001654168599032\n",
      "\n",
      "\n",
      "loss for fold 245: \n",
      " Regressor 1: 2.851993098140097\n",
      " Regressor 2: 0.06558555496995311\n",
      " Regressor 3: 0.5500383950300467\n",
      "\n",
      "\n",
      "loss for fold 246: \n",
      " Regressor 1: 7.204525476320047\n",
      " Regressor 2: 0.6825346850600553\n",
      " Regressor 3: 0.4930717950600556\n",
      "\n",
      "\n",
      "loss for fold 247: \n",
      " Regressor 1: 20.839335743080063\n",
      " Regressor 2: 2.4269984739200403\n",
      " Regressor 3: 2.6752186739200408\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The following script divides data into attributes and labels:\n",
    "X = dataset.iloc[:, :16].values\n",
    "y = dataset.iloc[:, 16:].values\n",
    "\n",
    "loss_per_fold_0 = []\n",
    "loss_per_fold_1 = []\n",
    "loss_per_fold_2 = []\n",
    "\n",
    "\n",
    "\n",
    "#Divide the data into training and testing sets for KFold Cross validaton loop\n",
    "# Define the K-fold Cross Validator\n",
    "cv = KFold(n_splits=247, shuffle=False)\n",
    "cv.get_n_splits(X)\n",
    "print(cv,\"\\n\")\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    #Scaling input\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "\n",
    "    #Traiing the algorithm\n",
    "    regressor = RandomForestRegressor(n_estimators=500, random_state=0)\n",
    "    regressor.fit(X_train, y_train)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "  \n",
    "    # Generate generalization metrics\n",
    "    \n",
    "    print(f'loss for fold {fold_no}: \\n Regressor 1: {metrics.mean_absolute_error(y_test[:,0], y_pred[:,0])}\\n Regressor 2: {metrics.mean_absolute_error(y_test[:,1], y_pred[:,1])}\\n Regressor 3: {metrics.mean_absolute_error(y_test[:,2], y_pred[:,1])}\\n\\n')\n",
    "\n",
    "    loss_per_fold_0.append(metrics.mean_absolute_error(y_test[:,0], y_pred[:,0]))\n",
    "    loss_per_fold_1.append(metrics.mean_absolute_error(y_test[:,1], y_pred[:,1]))\n",
    "    loss_per_fold_2.append(metrics.mean_absolute_error(y_test[:,2], y_pred[:,2]))\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "source": [
    "## Evaluating the Algorithm\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average scores for all folds:\n> Mean loss fisrt regressor: 1.7864409350067465\n> Mean loss fisrt regressor: 0.5094305614032772\n> Mean loss fisrt regressor: 0.5186803888110227\n------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print('Average scores for all folds:')\n",
    "print(f'> Mean loss fisrt regressor: {np.mean(loss_per_fold_0)}')\n",
    "print(f'> Mean loss Second regressor: {np.mean(loss_per_fold_1)}')\n",
    "print(f'> Mean loss Third regressor: {np.mean(loss_per_fold_2)}')\n",
    "print('------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}